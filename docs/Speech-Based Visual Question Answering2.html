<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Speech-Based Visual Question Answering| Hank's slides</title>
    <meta property="og:title" content="Speech-Based Visual Question Answering" />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="https://slides.hanklu.tw/featured-slide.jpg" />
    <meta property="og:url" content="https://slides.hanklu.tw" />
    <link rel="shortcut icon" href="./favicon.ico"/>
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="assets/theme/mytheme.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

# Speech-Based Visual Question Answering <!-- .element: class="title" -->
## Ted Zhang, Dengxin Dai, Tinne Tuytelaars, Marie-Francine Moens, Luc Van Gool<br> 2017 <!-- .element: class="subtitle" -->

<div class="title-name">
2021.02.02 <br>
Ke-Han Lu
</div>

https://arxiv.org/abs/1705.00464 <!-- .element: class="footnote" -->
</script></section><section  data-markdown><script type="text/template">
## Outline

- Task and Dataset
- Model
  - TextMod
  - SpeechMod
- Experiment
  - Noise
  - Zero-Shot
  - Human-recorded
- Discussion
- Future work
</script></section><section ><section data-markdown><script type="text/template">
## Task

**VQA**
- Question‚úèÔ∏è + ImageüñºÔ∏è ‚ûù Answer‚úèÔ∏è

**Speech-based VQA**
- Questionüîâ + ImageüñºÔ∏è ‚ûù Answer‚úèÔ∏è
- Vision + Text + Speech

![](attachments/2021-01-22-05-56-37.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Dataset (VQA)

<span>https://competitions.codalab.org/competitions/6961, https://visualqa.org/ </span> <!-- .element: class="footnote" -->

- VQA 1.0: 2015~2017
- VQA 2.0: 2017~present

<hr>

$\text{VQA 1.0}$

| Dataset       | Questions | Images | Answer      |
|---------------|-----------|--------|-------------|
| Train         | 248349    | 82783  | Available   |
| Val           | 121512    | 40504  | Available   |
| Test-dev      | 60864     | 81434  | Server only |
| Test-standard | 244302    | 81434  | Server only |
</script></section><section data-markdown><script type="text/template">
## Dataset (SpeechVQA1.0)

- Based on $\text{VQA 1.0}$
- Amazon Polly API: generate audio for each question
  - 200 hours synthetic audio
- 1000 human-recorded questions (not released)

</script></section></section><section ><section data-markdown><script type="text/template">
## Model

![](attachments/2021-01-22-03-20-57.png)  <!-- .element: class="img75" -->
</script></section><section data-markdown><script type="text/template">
## SpeechMod (CNN)

![](attachments/2021-02-01-22-02-26.png) <!-- .element: class="img100" -->
</script></section></section><section ><section data-markdown><script type="text/template">
## Experiment <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Data

- SpeechVQA 1.0
- Mix with different level of noise

$$
W_{\text{corrupted}} = (1-NL) * W_{\text{original}} + NL * W_{\text{noise}}
$$

- Real data: 1000 questions
  - $\frac{1}{3}$ male, $\frac{2}{3}$ female
</script></section><section data-markdown><script type="text/template">
## ASR

- Kaldi (DNN-HMM)

![](attachments/2021-01-22-04-27-48.png)

</script></section><section data-markdown><script type="text/template">
## Experiment

![](attachments/2021-02-01-21-39-59.png) <!-- .element: class="img50" -->

Baseline: LSTM Q+I (from $\text{VQA1.0}$)<!-- .element: class="footnote" -->
</script></section><section data-markdown><script type="text/template">
## Experiment (Blind)
- TextMod: train and evaluate on OQ
- SpeechMod: train and evaluate on 0% noise

![](attachments/2021-02-01-21-39-27.png) <!-- .element: class="img50" -->
</script></section><section data-markdown><script type="text/template">
## Zero-Shot
- Questions never seen in training
- $\text{ZS}$ is subset of $\text{val}$ set
- Trained on $\text{train}$, test on $\text{ZS}$
  - No test set because of no partial ground truth

![](attachments/2021-01-22-05-29-08.png)
</script></section><section data-markdown><script type="text/template">
## Human Recordings

- 1000 sample from $\text{val}$
- best model from $\text{ZS}$ were used for evaluation

![](attachments/2021-01-22-05-36-13.png) <!-- .element: class="img75" -->

All "yes": 29.66, 70.81, 0.39, 1.15 <!-- .element: class="footnote" -->
</script></section></section><section ><section data-markdown><script type="text/template">
## Discussion <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Discussion

- Speech contains more information than text
  - best performing model must be that which extract **patterns most effectively**
- TextMod: relies on ASR
  - ASR is more complicated than entire SpeechMod
  - ASR serve to filter out noise (a feature extractor, intermediate standardization of data)
- SpeechMod:  audio data only
  - the model may not extract the concept of words from audio (future research)
  - data standardization is helpful for unseen data
</script></section><section data-markdown><script type="text/template">
## Discussion.

(In Zero-Shot section)
- TextMod: can glimpse from word meaning
- SpeechMod: new words is entirely different signal!
  - continuous streams

(In human-recorded section)

- human-recorded audio has inflections, emphasis, accents, and pauses...
</script></section></section><section  data-markdown><script type="text/template">
## Future work

- Improve end-to-end model by data augmentation
- add feature extractors, attention mechanisms, GAN training...
- enforce prediction of question while learning to answer the question
- restrict the amount of training data to both approaches
  - compare learning efficiency
  - minor languages
</script></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"history":true,"center":false,"slideNumber":true,"mouseWheel":true,"transitionSpeed":"fast","transition":"none"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
