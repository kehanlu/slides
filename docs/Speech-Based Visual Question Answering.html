<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Speech-Based Visual Question Answering| Hank's slides</title>
    <meta property="og:title" content="Speech-Based Visual Question Answering" />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="https://slides.hanklu.tw/featured-slide.jpg" />
    <meta property="og:url" content="https://slides.hanklu.tw" />
    <link rel="shortcut icon" href="./favicon.ico"/>
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="assets/theme/mytheme.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

# Speech-Based Visual Question Answering <!-- .element: class="title" -->

<div class="title-name">
2021.01.26 <br>
Ke-Han Lu
</div>

https://arxiv.org/abs/1705.00464 <!-- .element: class="footnote" -->
</script></section><section  data-markdown><script type="text/template">
## Outline

- Task
- Model
  - TextMod
  - SpeechMod
- Experiment
  - Noise
  - Zero-Shot
  - Human-recorded
- Discussion
- Future work
</script></section><section ><section data-markdown><script type="text/template">
## Task

**VQA**
- Question‚úèÔ∏è + ImageüñºÔ∏è ‚ûù Answer‚úèÔ∏è

**Speech-based VQA**
- Questionüîâ + ImageüñºÔ∏è ‚ûù Answer‚úèÔ∏è
- Vision + Text + Speech + (Reasoning)

- Speech-based VQA can be used to **assist blind people** in performing ordinary tasks, and to
dictate robotics in real visual scenes in a **hand-free manner** such as
clinical robotic surgery
</script></section><section data-markdown><script type="text/template">
## Examples

![](attachments/2021-01-22-05-56-37.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## This work

- Investigates the potential of integrating vision and speech in the context of VQA

**2 methods**
- End-to-end: audio ‚ûù answer
  - Useful for languages that have less text-speech data.
- ASR: audio ‚ûù ASR ‚ûù Text-based method ‚ûù answer

</script></section><section data-markdown><script type="text/template">
## Main contributions

- Introduce an End-to-end model
- Inspect the performance impact of having different levels of noise
- Release a dataset: 200 hour synthetic and  1 hour real speech data.
</script></section></section><section ><section data-markdown><script type="text/template">
## Model

![](attachments/2021-01-22-03-20-57.png)  <!-- .element: class="img75" -->
</script></section><section data-markdown><script type="text/template">
## Language: TextMod

- Input: one-hot encodings
- Learned embedding layer
- LSTM + Dense

![](attachments/2021-01-22-03-25-31.png) <!-- .element: class="img25" -->
</script></section><section data-markdown><script type="text/template">
## Language: SpeechMod

- Input: raw waveform
- series of 1D convolutions (reduce dimensionality)
- LSTM + Dense

![](attachments/2021-01-22-03-26-48.png) <!-- .element: class="img25" -->

</script></section><section data-markdown><script type="text/template">
## Vision

- Input: Image(4096)
- Last layer of VGG19
- Dense
- Merge: using element-wise multiplication
- Dense output ‚ûù Probability distribution over N classes

![](attachments/2021-01-22-03-20-57.png)
</script></section><section data-markdown><script type="text/template">
## The architectures (WHY?)

- **Simplicity and similarity**
- Establish a baseline model for SVQA
- TextMod is similar to the baseline of VQA($\text{VQA1.0}$). SpeechMod also uses minimal components.
- TextMod and SpeechMod differ from each other as little as possible (for comparison)

</script></section></section><section ><section data-markdown><script type="text/template">
## Experiment <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Data

- VQA 1.0
- Amazon Polly API: generate audio file for each question
  - All same voice
- Mix with different level of noise

$$
W_{\text{corrupted}} = (1-NL) * W_{\text{original}} + NL * W_{\text{noise}}
$$

- Real data: 1000 questions
  - $\frac{1}{3}$ male, $\frac{2}{3}$ female
</script></section><section data-markdown><script type="text/template">
## ASR

- Kaldi (DNN-HMM)

![](attachments/2021-01-22-04-27-48.png)

</script></section><section data-markdown><script type="text/template">
## Experiment (Blind)
- TextMod: train and evaluate on OQ
- SpeechMod: train and evaluate on 0% noise

![](attachments/2021-01-22-04-45-14.png) <!-- .element: class="img50" -->
</script></section><section data-markdown><script type="text/template">
## Experiment (TextMod)
TextMod: trained on **original question**
- best performing model is used to evaluate based on transcribed text
- ASR: used 0-50% noise

![](attachments/2021-01-22-04-45-14.png) <!-- .element: class="img50" -->
</script></section><section data-markdown><script type="text/template">
## Experiment (SpeechMod)
SpeechMod: trained on 0% noise
- evaluate on 10~50% noise

![](attachments/2021-01-22-04-45-14.png) <!-- .element: class="img50" -->
</script></section><section data-markdown><script type="text/template">
## Experiment.
![](attachments/2021-01-22-05-12-13.png) <!-- .element: class="img100" -->
</script></section><section data-markdown><script type="text/template">
## Observation

- SpeechMod doesn't perform better
  - TextMod is 7% better than SpeechMod at 0% noise
- Compare to Blind models
  - Y/N Bias from dataset (the question is understood => good chance of answering correct answer)
  - Tells how many question are understood
  - Blind is better than 0% noise
  - Image + noise audio is less informative than Blind
</script></section><section data-markdown><script type="text/template">
## Zero-Shot
- Questions never seen in training
- $\text{ZS}$ is subset of $\text{val}$ set
- Trained on $\text{train}$, test on $\text{ZS}$
  - No test set because of no partial ground truth

![](attachments/2021-01-22-05-29-08.png)
</script></section><section data-markdown><script type="text/template">
## Human Recordings

- 1000 sample from $\text{val}$
- best model from $\text{ZS}$ were used for evaluation

![](attachments/2021-01-22-05-36-13.png) <!-- .element: class="img75" -->
</script></section></section><section ><section data-markdown><script type="text/template">
## Discussion <!-- .element: class="section-title" -->
</script></section><section data-markdown><script type="text/template">
## Discussion

- (modality) Speech contains more information than text
  - best performing model must be that which extract **patterns most effectively**
- TextMod: relies on ASR
  - ASR is more complicated than entire SpeechMod
  - ASR serve to filter out noise (a feature extractor, intermediate standardization of data)
- SpeechMod:  audio data only
  - the model may not extract the concept of words from audio (future research)
  - data standardization is helpful for unseen data
</script></section><section data-markdown><script type="text/template">
## Discussion.

(In Zero-Shot section)
- TextMod still better than SpeechMod
  - TextMod: can glimpse from word meaning
  - SpeechMod: new words is entirely different signal!
    - continuous streams

(In human-recorded section)

- human-recorded audio has inflections, emphasis, accents, and pauses...
</script></section><section data-markdown><script type="text/template">
## Discussion...

It is evident in our experiments that text-based VQA performs
better than speech-based, but **bearing in mind the simple architecture and limited amount of training data**, we believe the results of SpeechMod merits further study into end-to-end methods
</script></section></section><section  data-markdown><script type="text/template">
## Future work

- Improve end-to-end model by data augmentation
- add feature extractors, attention mechanisms, GAN training...
- enforce prediction of question while learning to answer the question

- restrict the amount of training data to both approaches
  - compare learning efficiency
  - minor languages
</script></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"history":true,"center":false,"slideNumber":true,"mouseWheel":true,"transitionSpeed":"fast"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
